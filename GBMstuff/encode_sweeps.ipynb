{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcb694f",
   "metadata": {},
   "source": [
    "# **Sweeps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee2fb9",
   "metadata": {},
   "source": [
    "## **Pre-Sweep**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77742a",
   "metadata": {},
   "source": [
    "### **Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4b3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8fbe8",
   "metadata": {},
   "source": [
    "### **Define Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd1468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, time_series_list, burst_ids):\n",
    "        self.time_series_list = time_series_list\n",
    "        self.burst_ids = burst_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_series_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.time_series_list[idx], self.burst_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11268f9b",
   "metadata": {},
   "source": [
    "### **Define Model Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Positional Encoding Class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Define the Channel Embedding Class\n",
    "class ChannelEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim):\n",
    "        super(ChannelEmbedding, self).__init__()\n",
    "        self.channel_embedding = nn.Linear(input_dim, model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_indices = torch.arange(input_dim).unsqueeze(0).expand(x.size(0), x.size(1), -1) \n",
    "        channel_embedded = self.channel_embedding(channel_indices.float())   \n",
    "        return x + channel_embedded    \n",
    "\n",
    "# Define the Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, model_dim),\n",
    "            nn.ReLU()  \n",
    "        )\n",
    "        self.channel_embedding = ChannelEmbedding(input_dim, model_dim)\n",
    "        self.positional_encoding = PositionalEncoding(model_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) \n",
    "        src = self.channel_embedding(src)        \n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output    \n",
    "\n",
    "# Define the Transformer Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, model_dim, output_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "\n",
    "# Define the Compression Neural Network\n",
    "class IntermediateNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, reduced_dim, sequence_length):\n",
    "        super(IntermediateNetwork, self).__init__()\n",
    "        self.reduce = nn.Sequential(\n",
    "            nn.Linear(sequence_length * latent_dim, reduced_dim),\n",
    "            nn.ReLU()  \n",
    "        )\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Linear(reduced_dim, sequence_length * latent_dim),\n",
    "            nn.ReLU() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reduce dimensionality\n",
    "        org_shape = x.size()\n",
    "        x = x.reshape(x.size(1), -1)  # Reshape to (batch_size, sequence_length * latent_dim)\n",
    "        reduced_x = self.reduce(x)\n",
    "\n",
    "        # Expand dimensionality\n",
    "        expanded_x = self.expand(reduced_x)\n",
    "        expanded_x = expanded_x.reshape(org_shape)  # Reshape back\n",
    "\n",
    "        return expanded_x, reduced_x \n",
    "    \n",
    "# Define the Autoencoder\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, sequence_length, reduced_dim, dropout=0.1):  \n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        self.encoder = TransformerEncoder(input_dim, model_dim, num_heads, num_layers, dropout)\n",
    "        self.intermediate_network = IntermediateNetwork(model_dim, reduced_dim, sequence_length)\n",
    "        self.decoder = TransformerDecoder(model_dim, input_dim, num_heads, num_layers, dropout)  \n",
    "\n",
    "    def forward(self, src):\n",
    "        memory = self.encoder(src)\n",
    "        expanded_latent, reduced_latent = self.intermediate_network(memory)  \n",
    "        tgt = self.encoder.embedding(src)  \n",
    "        tgt = self.encoder.positional_encoding(tgt)  \n",
    "        output = self.decoder(tgt, expanded_latent)  \n",
    "        return output, reduced_latent  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79f179",
   "metadata": {},
   "source": [
    "### **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c03b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    lcs = pd.read_csv('lcs.csv')\n",
    "    channels = ['n0','n1','n2','n3','n4','n5','n6','n7','n8','n9','na','nb','b0','b1']\n",
    "\n",
    "    for ch in channels:\n",
    "        missing = lcs[ch].isnull()\n",
    "        lcs.loc[missing, ch] = np.random.normal(lcs[ch].mean(), lcs[ch].std(), missing.sum())\n",
    "\n",
    "    time_series_list, burst_ids = [], []\n",
    "    for burst, group in lcs.groupby('burst'):\n",
    "        data = group[channels].values\n",
    "        time_series_list.append(torch.tensor(data, dtype=torch.float32))\n",
    "        burst_ids.append(burst)\n",
    "\n",
    "    time_series_list = nn.utils.rnn.pad_sequence(time_series_list, batch_first=True, padding_value=0.0)\n",
    "    scaler = StandardScaler()\n",
    "    flat = time_series_list.reshape(time_series_list.shape[0], -1)\n",
    "    flat = scaler.fit_transform(flat)\n",
    "    time_series_list = flat.reshape(time_series_list.shape)\n",
    "    return time_series_list, burst_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f632f",
   "metadata": {},
   "source": [
    "## **Sweep**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb2a2a",
   "metadata": {},
   "source": [
    "### **Sweep Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542d86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_sweep():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    # Load data\n",
    "    time_series_list, burst_ids = load_data()\n",
    "    dataset = TimeSeriesDataset(time_series_list, burst_ids)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    seq_len = time_series_list.shape[1]\n",
    "    model = TransformerAutoencoder(\n",
    "        input_dim=14,\n",
    "        model_dim=config.model_dim,\n",
    "        num_heads=config.num_heads,\n",
    "        num_layers=config.num_layers,\n",
    "        sequence_length=seq_len,\n",
    "        reduced_dim=config.reduced_dim,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        for batch, _ in dataloader:\n",
    "            batch = batch.permute(1, 0, 2).float()\n",
    "            reconstructed, _ = model(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss.item(),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "    wandb.log({\"final_loss\": loss.item()})\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e06fec",
   "metadata": {},
   "source": [
    "### **Sweep Config** - _THE IMPORTANT PART_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7fbae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sweeps = 20\n",
    "\n",
    "# Flags to toggle which hyperparameters to sweep\n",
    "config_flags = {\n",
    "    'sweep_model_dim': False,\n",
    "    'sweep_num_heads': False,\n",
    "    'sweep_num_layers': False,\n",
    "    'sweep_reduced_dim': False,\n",
    "    'sweep_batch_size': False,\n",
    "    'sweep_learning_rate': True,\n",
    "    'sweep_dropout': False,\n",
    "    'sweep_method': 'bayes'  # 'random', 'bayes', or 'grid'\n",
    "}\n",
    "\n",
    "# Default values if not swept\n",
    "fixed_defaults = {\n",
    "    'input_dim': 14,\n",
    "    'model_dim': 32,\n",
    "    'num_heads': 4,\n",
    "    'num_layers': 2,\n",
    "    'reduced_dim': 1024,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.0001,\n",
    "    'dropout': 0.1,\n",
    "    'num_epochs': 20\n",
    "    # 'sequence_length' will be set dynamically from data\n",
    "}\n",
    "\n",
    "def generate_sweep_config(flags, defaults):\n",
    "    sweep_config = {\n",
    "        'method': flags['sweep_method'],\n",
    "        'metric': {'name': 'loss', 'goal': 'minimize'},\n",
    "        'parameters': {}\n",
    "    }\n",
    "\n",
    "    if flags['sweep_model_dim']:\n",
    "        sweep_config['parameters']['model_dim'] = {'values': [32, 64, 128]}\n",
    "    else:\n",
    "        sweep_config['parameters']['model_dim'] = {'value': defaults['model_dim']}\n",
    "\n",
    "    if flags['sweep_num_heads']:\n",
    "        sweep_config['parameters']['num_heads'] = {'values': [2, 4, 8]}\n",
    "    else:\n",
    "        sweep_config['parameters']['num_heads'] = {'value': defaults['num_heads']}\n",
    "\n",
    "    if flags['sweep_num_layers']:\n",
    "        sweep_config['parameters']['num_layers'] = {'values': [1, 2, 3]}\n",
    "    else:\n",
    "        sweep_config['parameters']['num_layers'] = {'value': defaults['num_layers']}\n",
    "\n",
    "    if flags['sweep_reduced_dim']:\n",
    "        sweep_config['parameters']['reduced_dim'] = {'values': [256, 512, 1024]}\n",
    "    else:\n",
    "        sweep_config['parameters']['reduced_dim'] = {'value': defaults['reduced_dim']}\n",
    "\n",
    "    if flags['sweep_batch_size']:\n",
    "        sweep_config['parameters']['batch_size'] = {'values': [8, 16, 32]}\n",
    "    else:\n",
    "        sweep_config['parameters']['batch_size'] = {'value': defaults['batch_size']}\n",
    "\n",
    "    if flags['sweep_learning_rate']:\n",
    "        sweep_config['parameters']['learning_rate'] = {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-2\n",
    "        }\n",
    "    else:\n",
    "        sweep_config['parameters']['learning_rate'] = {'value': defaults['learning_rate']}\n",
    "\n",
    "    if flags['sweep_dropout']:\n",
    "        sweep_config['parameters']['dropout'] = {'values': [0.0, 0.1, 0.2]}\n",
    "    else:\n",
    "        sweep_config['parameters']['dropout'] = {'value': defaults['dropout']}\n",
    "\n",
    "    sweep_config['parameters']['num_epochs'] = {'value': defaults['num_epochs']}\n",
    "\n",
    "    return sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806476b2",
   "metadata": {},
   "source": [
    "### **Run Sweep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38992e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: lf1kv01s\n",
      "Sweep URL: https://wandb.ai/tobiassafie-drexel-university/GBM-LSTM-Sweep/sweeps/lf1kv01s\n",
      " lf1kv01s\n",
      "Sweep URL: https://wandb.ai/tobiassafie-drexel-university/GBM-LSTM-Sweep/sweeps/lf1kv01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: gbuyuxsm with config:\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tdropout: 0.1\n",
      "wandb: \tlearning_rate: 0.00193007480382725\n",
      "wandb: \tmodel_dim: 32\n",
      "wandb: \tnum_epochs: 20\n",
      "wandb: \tnum_heads: 4\n",
      "wandb: \tnum_layers: 2\n",
      "wandb: \treduced_dim: 1024\n",
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tdropout: 0.1\n",
      "wandb: \tlearning_rate: 0.00193007480382725\n",
      "wandb: \tmodel_dim: 32\n",
      "wandb: \tnum_epochs: 20\n",
      "wandb: \tnum_heads: 4\n",
      "wandb: \tnum_layers: 2\n",
      "wandb: \treduced_dim: 1024\n",
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: tobiassafie (tobiassafie-drexel-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: tobiassafie (tobiassafie-drexel-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sweep_config = generate_sweep_config(config_flags, fixed_defaults)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"GBM-LSTM-Sweep\")\n",
    "wandb.agent(sweep_id, function=train_autoencoder_sweep, count=num_sweeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16609865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "star-pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
